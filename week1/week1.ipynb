{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a5a1015-19c0-4f88-96d8-834086de7706",
   "metadata": {},
   "source": [
    "# Tuần 1\n",
    "\n",
    "## Câu hỏi:\n",
    "\n",
    "### NLP (Xử lý ngôn ngữ tự nhiên) là gì?\n",
    "NLP là một lĩnh vực liên ngành sôi động với nhiều tên gọi tương ứng với nhiều khía cạnh của nó, như:\n",
    "- Xử lý ngôn ngữ và giọng nói\n",
    "- Công nghệ ngôn ngữ của con người\n",
    "- Xử lý ngôn ngữ tự nhiên\n",
    "- Ngôn ngữ học tính toán\n",
    "- Nhận dạng và tổng hợp giọng nói\n",
    "Mục tiêu của lĩnh vực này là làm cho máy tính thực hiện các tác vụ hữu ích liên quan đến ngôn ngữ của con người.\n",
    "\n",
    "### Tại sao NLP lại quan trọng?\n",
    "NLP quan trọng vì nó cho phép máy tính thực hiện các tác vụ hữu ích liên quan đến ngôn ngữ của con người, chẳng hạn như:\n",
    "\n",
    "- **Cho phép giao tiếp giữa người và máy:**\n",
    "\n",
    "- **Cải thiện giao tiếp giữa người với người:**\n",
    "\n",
    "- **Cho phép xử lý văn bản và giọng nói một cách hữu ích:**\n",
    "\n",
    "Một số ứng dụng của NLP:\n",
    "\n",
    "- **Tác nhân hội thoại (Conversational agents):** Như HAL 9000, có thể nói và hiểu tiếng Anh, đọc khẩu hình.\n",
    "\n",
    "- **Dịch máy (Machine translation):** Tự động dịch tài liệu từ ngôn ngữ này sang ngôn ngữ khác.\n",
    "\n",
    "- **Trả lời câu hỏi dựa trên Web (Web-based question answering):** Trả lời các câu hỏi phức tạp hơn so với tìm kiếm web thông thường.\n",
    "\n",
    "---\n",
    "\n",
    "### Các bước phổ biến trong tiền xử lý văn bản:\n",
    "\n",
    "#### 1. Tokenization (Phân tách thành token)\n",
    "Chia một chuỗi văn bản thành các đơn vị nhỏ hơn (từ hoặc phần của từ).\n",
    "\n",
    "Hai phương pháp tokenization:\n",
    "- **Top-down (rule-based) tokenization:** Dựa trên các quy tắc xác định trước.\n",
    "- **Bottom-up tokenization (subword tokens):** Chia từ dựa trên thống kê (ví dụ: Byte-Pair Encoding - BPE).\n",
    "\n",
    "#### 2. Normalization (Chuẩn hóa)\n",
    "Đưa các từ về định dạng tiêu chuẩn.\n",
    "\n",
    "Các kỹ thuật chuẩn hóa:\n",
    "- **Case folding:** Chuyển tất cả chữ cái về chữ thường.\n",
    "- **Stemming:** Loại bỏ hậu tố của từ để đưa về gốc.\n",
    "- **Lemmatization:** Xác định dạng từ điển của từ, yêu cầu phân tích hình thái học phức tạp hơn stemming.\n",
    "\n",
    "Hai lỗi phổ biến trong stemming:\n",
    "- **Over-stemming:** Hai từ có nguồn gốc khác nhau nhưng bị chuyển về cùng một gốc.\n",
    "- **Under-stemming:** Hai từ có cùng gốc nhưng không được chuyển về cùng một gốc.\n",
    "\n",
    "#### 3. Sentence Segmentation (Phân đoạn câu)\n",
    "Chia văn bản thành các câu riêng biệt.\n",
    "\n",
    "Các phương pháp thường dựa vào dấu câu và từ điển viết tắt để phân biệt dấu kết thúc câu.\n",
    "\n",
    "#### 4. Remove stop-word\n",
    "Stop words là những từ thường xuyên xuất hiện (\"the\", \"a\", \"an\", \"in\") nhưng không mang nhiều giá trị ngữ nghĩa.\n",
    "\n",
    "**Tại sao cần loại bỏ stop words?**\n",
    "- **Phân loại văn bản:** Loại bỏ stop words giúp tập trung vào từ quan trọng.\n",
    "- **Dịch máy & tóm tắt văn bản:** Không nên loại bỏ stop words vì chúng quan trọng để giữ nguyên ý nghĩa.\n",
    "\n",
    "Stop words có thể bị loại bỏ để tiết kiệm tài nguyên và tối ưu hóa xử lý dữ liệu.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b54a545d0734404a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T11:13:12.577386Z",
     "start_time": "2025-02-05T11:13:11.524187Z"
    }
   },
   "source": [
    "# Nhiệm vụ: Xây dựng một pipeline tiền xử lý văn bản đơn giản.\n",
    "# - Công cụ: Python với các thư viện như NLTK hoặc spaCy.\n",
    "# - Ví dụ: Tải một tập dữ liệu mẫu (ví dụ: một tập hợp các bài báo tin tức hoặc tweet),\n",
    "# token hóa văn bản, loại bỏ stop-word và thực hiện stemming hoặc lemmatization.\n",
    "# - Kết quả: Hiểu rõ cách dữ liệu thô được chuyển đổi thành dữ liệu có cấu trúc.\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load spacy english model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Sample text\n",
    "f = open('./textdata.txt', 'r')\n",
    "text = f.read()\n",
    "f.close()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "bcf9e082ce5c2d04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T11:13:15.175915Z",
     "start_time": "2025-02-05T11:13:15.137800Z"
    }
   },
   "source": [
    "# tokenize text with spacy\n",
    "print('Tokenize word:')\n",
    "doc = nlp(text)\n",
    "print([token.text for token in doc])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize word:\n",
      "['Welcome', 'to', 'Natural', 'Language', 'Processing', ' \\n', 'It', 'is', 'one', 'of', 'the', 'most', 'exciting', 'research', 'areas', 'as', 'of', 'today', ' \\n', 'We', 'will', 'see', 'how', 'Python', 'can', 'be', 'used', 'to', 'work', 'with', 'text', 'files', '.']\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "5b30a5af-d0aa-4c49-af3e-2a996c3162fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T11:14:11.703503Z",
     "start_time": "2025-02-05T11:14:11.695399Z"
    }
   },
   "source": [
    "# remove stop-word with spacy\n",
    "print('Remove stop-word:')\n",
    "filtered_doc = [token for token in doc if not token.is_stop]\n",
    "print([token.text for token in filtered_doc])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove stop-word:\n",
      "['Welcome', 'Natural', 'Language', 'Processing', ' \\n', 'exciting', 'research', 'areas', 'today', ' \\n', 'Python', 'work', 'text', 'files', '.']\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "7d10c86b488ac943",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-05T11:14:14.380511Z",
     "start_time": "2025-02-05T11:14:14.372751Z"
    }
   },
   "source": [
    "# lemmatization with spacy\n",
    "print('Lemmatization original doc:')\n",
    "lemmetization_doc = [token.lemma_ for token in doc]\n",
    "print(lemmetization_doc)\n",
    "\n",
    "print('Lemmatization doc:')\n",
    "lemmetization_filtered_doc = [token.lemma_ for token in filtered_doc]\n",
    "print(lemmetization_filtered_doc)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization original doc:\n",
      "['welcome', 'to', 'Natural', 'Language', 'Processing', ' \\n', 'it', 'be', 'one', 'of', 'the', 'most', 'exciting', 'research', 'area', 'as', 'of', 'today', ' \\n', 'we', 'will', 'see', 'how', 'Python', 'can', 'be', 'use', 'to', 'work', 'with', 'text', 'file', '.']\n",
      "Lemmatization doc:\n",
      "['welcome', 'Natural', 'Language', 'Processing', ' \\n', 'exciting', 'research', 'area', 'today', ' \\n', 'Python', 'work', 'text', 'file', '.']\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
